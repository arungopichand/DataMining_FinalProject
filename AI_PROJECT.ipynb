{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import json\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOADING THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    f = open(filename, 'r')\n",
    "    \n",
    "    df = pd.read_json(f, lines=True)\n",
    "    \n",
    "    df.drop(columns = ['article_link'])                    #drop the column 'article_link', as we only need the headline body\n",
    "    del df['article_link']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26704</th>\n",
       "      <td>american politics in moral free-fall</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26705</th>\n",
       "      <td>america's best 20 hikes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26706</th>\n",
       "      <td>reparations and obama</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26707</th>\n",
       "      <td>israeli ban targeting boycott supporters raise...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26708</th>\n",
       "      <td>gourmet gifts for the foodie 2014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26709 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                headline  is_sarcastic\n",
       "0      former versace store clerk sues over secret 'b...             0\n",
       "1      the 'roseanne' revival catches up to our thorn...             0\n",
       "2      mom starting to fear son's web series closest ...             1\n",
       "3      boehner just wants wife to listen, not come up...             1\n",
       "4      j.k. rowling wishes snape happy birthday in th...             0\n",
       "...                                                  ...           ...\n",
       "26704               american politics in moral free-fall             0\n",
       "26705                            america's best 20 hikes             0\n",
       "26706                              reparations and obama             0\n",
       "26707  israeli ban targeting boycott supporters raise...             0\n",
       "26708                  gourmet gifts for the foodie 2014             0\n",
       "\n",
       "[26709 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileName = 'Sarcasm_Headlines_Dataset.json'\n",
    "\n",
    "data = read_data(fileName) # Read Data file\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences in dataset: 26709\n"
     ]
    }
   ],
   "source": [
    "print('Total sentences in dataset:', len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sentences in dataset with sarcastic tag: 14985\n",
      "Total Sentences in dataset with non-sarcastic tag: 11724\n"
     ]
    }
   ],
   "source": [
    "print('Total Sentences in dataset with sarcastic tag:', len(data[data['is_sarcastic'] == 0]))\n",
    "print('Total Sentences in dataset with non-sarcastic tag:', len(data[data['is_sarcastic'] == 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Process data \n",
    "Removing all the stop words from the text and finding the lemma(perform lemmatization) of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method to Remove all the punctuation symbols from a sentence \n",
    "\n",
    "import re\n",
    "\n",
    "def eliminate_punct(sent):\n",
    "    sent = re.sub(\"[']\", '', sent)\n",
    "    sent = re.sub(\"[^\\w]\", ' ', sent)\n",
    "    \n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the lemmatization to each word in the dataset\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(sentence):\n",
    "    words = sentence.split(' ')\n",
    "    new_sent = \"\"\n",
    "\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            word = lemmatizer.lemmatize(word)\n",
    "            new_sent += word\n",
    "            new_sent += \" \"\n",
    "    \n",
    "    return new_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(data):\n",
    "    for i in range(0, len(data)):\n",
    "        sentence = eliminate_punct(data.iloc[i,0])\n",
    "        #sentence = remove_stop_words(sentence)\n",
    "       \n",
    "        data.iloc[i,0] = sentence.lower()\n",
    "    return data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = process(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Data After PreProcessing\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>former versace store clerk sues over secret bl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the roseanne revival catches up to our thorny ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mom starting to fear sons web series closest t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boehner just wants wife to listen  not come up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>j k  rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26704</th>\n",
       "      <td>american politics in moral free fall</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26705</th>\n",
       "      <td>americas best 20 hikes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26706</th>\n",
       "      <td>reparations and obama</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26707</th>\n",
       "      <td>israeli ban targeting boycott supporters raise...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26708</th>\n",
       "      <td>gourmet gifts for the foodie 2014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26709 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                headline  is_sarcastic\n",
       "0      former versace store clerk sues over secret bl...             0\n",
       "1      the roseanne revival catches up to our thorny ...             0\n",
       "2      mom starting to fear sons web series closest t...             1\n",
       "3      boehner just wants wife to listen  not come up...             1\n",
       "4      j k  rowling wishes snape happy birthday in th...             0\n",
       "...                                                  ...           ...\n",
       "26704               american politics in moral free fall             0\n",
       "26705                             americas best 20 hikes             0\n",
       "26706                              reparations and obama             0\n",
       "26707  israeli ban targeting boycott supporters raise...             0\n",
       "26708                  gourmet gifts for the foodie 2014             0\n",
       "\n",
       "[26709 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('The Data After PreProcessing')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Tf-Vector (Term Frequency Vector)\n",
    "The TF vector will be used for the VSM model as well as for eliminating the words with low frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This method would return the word with frequency greater than 5\n",
    "\n",
    "def high_freq_words(tf_vector):\n",
    "    final_tf = {}\n",
    "    \n",
    "    for word in tf_vector:\n",
    "        if tf_vector[word] >= 5:\n",
    "            final_tf[word] = tf_vector[word]\n",
    "    \n",
    "    return final_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_tf(data):\n",
    "    tf_words = {}\n",
    "    \n",
    "    for i in range(len(data['headline'])):\n",
    "        line = data['headline'][i].split()\n",
    "        \n",
    "        for word in line:\n",
    "            if word in tf_words:\n",
    "                tf_words[word] += 1\n",
    "            else:\n",
    "                tf_words[word] = 1\n",
    "    \n",
    "    return high_freq_words(tf_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Low Freq Words\n",
    "words_tf = calc_tf(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'former': 106,\n",
       " 'store': 55,\n",
       " 'clerk': 11,\n",
       " 'sues': 17,\n",
       " 'over': 483,\n",
       " 'secret': 92,\n",
       " 'black': 267,\n",
       " 'code': 19,\n",
       " 'for': 3321,\n",
       " 'minority': 15,\n",
       " 'the': 5415,\n",
       " 'revival': 11,\n",
       " 'catches': 14,\n",
       " 'up': 842,\n",
       " 'to': 8267,\n",
       " 'our': 177,\n",
       " 'political': 83,\n",
       " 'mood': 13,\n",
       " 'better': 124,\n",
       " 'and': 1935,\n",
       " 'worse': 40,\n",
       " 'mom': 198,\n",
       " 'starting': 45,\n",
       " 'fear': 46,\n",
       " 'sons': 36,\n",
       " 'web': 19,\n",
       " 'series': 62,\n",
       " 'closest': 7,\n",
       " 'thing': 144,\n",
       " 'she': 199,\n",
       " 'will': 558,\n",
       " 'have': 494,\n",
       " 'boehner': 27,\n",
       " 'just': 579,\n",
       " 'wants': 139,\n",
       " 'wife': 84,\n",
       " 'listen': 24,\n",
       " 'not': 739,\n",
       " 'come': 105,\n",
       " 'with': 1805,\n",
       " 'alternative': 13,\n",
       " 'debt': 29,\n",
       " 'ideas': 43,\n",
       " 'j': 49,\n",
       " 'k': 53,\n",
       " 'rowling': 7,\n",
       " 'wishes': 42,\n",
       " 'happy': 64,\n",
       " 'birthday': 64,\n",
       " 'in': 4238,\n",
       " 'most': 261,\n",
       " 'magical': 32,\n",
       " 'way': 266,\n",
       " 'worlds': 93,\n",
       " 'women': 329,\n",
       " 'case': 94,\n",
       " 'eating': 76,\n",
       " 'lab': 8,\n",
       " 'grown': 18,\n",
       " 'meat': 28,\n",
       " 'this': 920,\n",
       " 'ceo': 73,\n",
       " 'send': 35,\n",
       " 'your': 653,\n",
       " 'kids': 197,\n",
       " 'school': 247,\n",
       " 'if': 301,\n",
       " 'you': 1004,\n",
       " 'work': 187,\n",
       " 'his': 633,\n",
       " 'company': 94,\n",
       " 'top': 126,\n",
       " 'snake': 10,\n",
       " 'leaves': 65,\n",
       " 'sinking': 6,\n",
       " 'huckabee': 20,\n",
       " 'campaign': 196,\n",
       " 'fridays': 19,\n",
       " 'morning': 101,\n",
       " 'email': 95,\n",
       " 'inside': 83,\n",
       " 'trumps': 391,\n",
       " 'ages': 9,\n",
       " 'airline': 13,\n",
       " 'passengers': 25,\n",
       " 'tackle': 7,\n",
       " 'man': 1266,\n",
       " 'who': 576,\n",
       " 'rushes': 8,\n",
       " 'bomb': 22,\n",
       " 'threat': 37,\n",
       " 'facebook': 112,\n",
       " 'reportedly': 48,\n",
       " 'working': 78,\n",
       " 'on': 2433,\n",
       " 'healthcare': 26,\n",
       " 'features': 21,\n",
       " 'apps': 10,\n",
       " 'north': 130,\n",
       " 'korea': 69,\n",
       " 'praises': 13,\n",
       " 'trump': 1295,\n",
       " 'urges': 41,\n",
       " 'us': 147,\n",
       " 'voters': 94,\n",
       " 'reject': 8,\n",
       " 'hillary': 207,\n",
       " 'actually': 134,\n",
       " 'cnns': 9,\n",
       " 'lord': 14,\n",
       " 'has': 560,\n",
       " 'been': 130,\n",
       " 'a': 3093,\n",
       " 'while': 167,\n",
       " 'holds': 29,\n",
       " 'huge': 58,\n",
       " 'protest': 57,\n",
       " 'support': 102,\n",
       " 'of': 5634,\n",
       " 'refugees': 43,\n",
       " 'nuclear': 67,\n",
       " 'during': 216,\n",
       " 'spider': 14,\n",
       " 'musical': 22,\n",
       " 'cosby': 15,\n",
       " 'lawyer': 30,\n",
       " 'asks': 88,\n",
       " 'why': 506,\n",
       " 'accusers': 10,\n",
       " 'didnt': 96,\n",
       " 'forward': 39,\n",
       " 'be': 826,\n",
       " 'by': 896,\n",
       " 'legal': 36,\n",
       " 'team': 89,\n",
       " 'years': 286,\n",
       " 'ago': 31,\n",
       " 'stock': 25,\n",
       " 'confused': 12,\n",
       " 'market': 33,\n",
       " 'program': 45,\n",
       " 'build': 23,\n",
       " 'cities': 34,\n",
       " 'got': 145,\n",
       " 'bigger': 14,\n",
       " 'craig': 10,\n",
       " 'hicks': 8,\n",
       " 'indicted': 12,\n",
       " 'sketch': 9,\n",
       " 'artist': 46,\n",
       " 'clear': 28,\n",
       " 'assures': 29,\n",
       " 'nation': 211,\n",
       " 'that': 778,\n",
       " 'decision': 49,\n",
       " 'syrian': 51,\n",
       " 'airstrikes': 6,\n",
       " 'came': 33,\n",
       " 'after': 851,\n",
       " 'carefully': 11,\n",
       " 'considering': 16,\n",
       " 'all': 585,\n",
       " 'passing': 22,\n",
       " 'qatar': 9,\n",
       " 'dutch': 5,\n",
       " 'woman': 444,\n",
       " 'reported': 13,\n",
       " 'was': 323,\n",
       " 'raped': 7,\n",
       " 'is': 1685,\n",
       " 'shouldnt': 34,\n",
       " 'go': 164,\n",
       " 'circus': 5,\n",
       " 'ted': 66,\n",
       " 'cruz': 72,\n",
       " 'hits': 50,\n",
       " 'panic': 14,\n",
       " 'button': 11,\n",
       " 'we': 312,\n",
       " 'could': 281,\n",
       " 'lose': 46,\n",
       " 'both': 20,\n",
       " 'houses': 14,\n",
       " 'congress': 134,\n",
       " 'writers': 14,\n",
       " 'must': 103,\n",
       " 'plan': 147,\n",
       " 'surprised': 32,\n",
       " 'obama': 362,\n",
       " 'visits': 23,\n",
       " 'national': 129,\n",
       " 'cemetery': 5,\n",
       " 'honor': 35,\n",
       " 'veterans': 21,\n",
       " 'ex': 75,\n",
       " 'con': 13,\n",
       " 'back': 326,\n",
       " 'behind': 94,\n",
       " 'bar': 60,\n",
       " 'careful': 5,\n",
       " 'bush': 149,\n",
       " 'recommends': 28,\n",
       " 'oil': 50,\n",
       " 'drilling': 9,\n",
       " 'beginning': 23,\n",
       " 'task': 5,\n",
       " 'allies': 15,\n",
       " 'islamist': 5,\n",
       " 'killing': 51,\n",
       " 'nonsense': 11,\n",
       " 'what': 641,\n",
       " 'its': 421,\n",
       " 'like': 407,\n",
       " 'kiss': 12,\n",
       " 'adam': 19,\n",
       " 'uber': 30,\n",
       " 'vows': 49,\n",
       " 'nyc': 44,\n",
       " 'drivers': 17,\n",
       " 'millions': 54,\n",
       " 'tax': 95,\n",
       " 'apple': 50,\n",
       " 'may': 201,\n",
       " 'electric': 17,\n",
       " 'motorcycle': 9,\n",
       " 'death': 223,\n",
       " 'drug': 71,\n",
       " 'resistant': 6,\n",
       " 'bacteria': 11,\n",
       " 'often': 8,\n",
       " 'childrens': 24,\n",
       " 'dogs': 41,\n",
       " 'see': 164,\n",
       " 'muslim': 65,\n",
       " 'at': 1205,\n",
       " 'airport': 36,\n",
       " 'giant': 34,\n",
       " 'heading': 7,\n",
       " 'toward': 38,\n",
       " 'earth': 67,\n",
       " 'straight': 61,\n",
       " 'box': 58,\n",
       " 'office': 151,\n",
       " 'massive': 51,\n",
       " '1': 142,\n",
       " 'million': 143,\n",
       " 'opening': 42,\n",
       " 'money': 126,\n",
       " 'baseball': 24,\n",
       " 'lost': 73,\n",
       " 'next': 159,\n",
       " 'generation': 24,\n",
       " 'fans': 94,\n",
       " 'robin': 12,\n",
       " 'williams': 45,\n",
       " 'holiday': 63,\n",
       " 'eighth': 7,\n",
       " 'year': 513,\n",
       " 'devin': 5,\n",
       " 'nunes': 6,\n",
       " 'never': 170,\n",
       " 'reveal': 41,\n",
       " 'source': 17,\n",
       " 'surveillance': 11,\n",
       " 'claims': 82,\n",
       " 'scott': 57,\n",
       " 'used': 91,\n",
       " 'stop': 161,\n",
       " 'breathing': 11,\n",
       " 'nearly': 36,\n",
       " '40': 51,\n",
       " 'times': 87,\n",
       " 'an': 375,\n",
       " 'hour': 63,\n",
       " 'device': 10,\n",
       " 'changed': 27,\n",
       " 'life': 376,\n",
       " 'rescuers': 5,\n",
       " 'help': 141,\n",
       " 'garbage': 12,\n",
       " 'into': 515,\n",
       " 'ocean': 18,\n",
       " 'drop': 30,\n",
       " 'soccer': 26,\n",
       " 'player': 34,\n",
       " 'from': 1226,\n",
       " 'hes': 157,\n",
       " 'give': 123,\n",
       " 'gift': 57,\n",
       " 'play': 59,\n",
       " 'season': 126,\n",
       " 'christian': 38,\n",
       " 'sikh': 9,\n",
       " 'temple': 5,\n",
       " 'victims': 59,\n",
       " 'spicer': 18,\n",
       " 'denies': 19,\n",
       " 'ending': 18,\n",
       " 'maternity': 5,\n",
       " 'care': 139,\n",
       " 'would': 238,\n",
       " 'mean': 39,\n",
       " 'pay': 76,\n",
       " 'more': 544,\n",
       " 'health': 210,\n",
       " 'right': 197,\n",
       " 'live': 141,\n",
       " 'complete': 30,\n",
       " 'stunned': 10,\n",
       " 'horror': 22,\n",
       " 'added': 12,\n",
       " 'constitution': 16,\n",
       " 'nasa': 34,\n",
       " 'now': 361,\n",
       " 'almost': 51,\n",
       " 'positive': 22,\n",
       " 'mars': 22,\n",
       " 'rocky': 9,\n",
       " 'monster': 14,\n",
       " 'night': 136,\n",
       " 'light': 52,\n",
       " 'diy': 6,\n",
       " 'sports': 54,\n",
       " 'closet': 14,\n",
       " 'dead': 184,\n",
       " '3': 242,\n",
       " 'injured': 26,\n",
       " 'shooting': 120,\n",
       " 't': 65,\n",
       " 'i': 381,\n",
       " 'concert': 26,\n",
       " 'longtime': 14,\n",
       " 'teacher': 85,\n",
       " 'retires': 8,\n",
       " 'without': 116,\n",
       " 'changing': 34,\n",
       " 'single': 108,\n",
       " 'students': 128,\n",
       " 'donald': 472,\n",
       " 'wins': 71,\n",
       " 'polls': 29,\n",
       " 'say': 211,\n",
       " 'wouldnt': 23,\n",
       " 'had': 120,\n",
       " 'ready': 76,\n",
       " 'cash': 27,\n",
       " 'self': 177,\n",
       " 'finance': 7,\n",
       " 'entire': 106,\n",
       " 'analysis': 7,\n",
       " 'new': 1523,\n",
       " 'star': 164,\n",
       " 'wars': 51,\n",
       " 'film': 108,\n",
       " 'once': 91,\n",
       " 'again': 151,\n",
       " 'die': 58,\n",
       " 'hard': 68,\n",
       " 'bats': 6,\n",
       " 'out': 838,\n",
       " 'nations': 161,\n",
       " 'summer': 77,\n",
       " 'mobile': 6,\n",
       " 'news': 174,\n",
       " 'crew': 23,\n",
       " 'reports': 79,\n",
       " 'own': 166,\n",
       " 'van': 19,\n",
       " 'breaking': 74,\n",
       " 'down': 310,\n",
       " 'un': 52,\n",
       " 'rights': 96,\n",
       " 'chief': 72,\n",
       " 'calls': 173,\n",
       " 'situation': 15,\n",
       " 'syria': 54,\n",
       " 'outrage': 8,\n",
       " 'how': 817,\n",
       " 'track': 35,\n",
       " 'santa': 26,\n",
       " 'claus': 5,\n",
       " 'flight': 38,\n",
       " 'around': 143,\n",
       " 'world': 291,\n",
       " 'christmas': 98,\n",
       " 'eve': 15,\n",
       " 'colorado': 21,\n",
       " 'trouble': 18,\n",
       " 'tvs': 6,\n",
       " 'africa': 20,\n",
       " 'unveils': 80,\n",
       " 'throat': 9,\n",
       " 'water': 75,\n",
       " 'report': 515,\n",
       " 'john': 170,\n",
       " 'slowly': 28,\n",
       " 'but': 228,\n",
       " 'list': 76,\n",
       " 'greatest': 30,\n",
       " 'living': 79,\n",
       " 'american': 250,\n",
       " 'authors': 6,\n",
       " 'ghost': 16,\n",
       " 'cant': 225,\n",
       " 'make': 333,\n",
       " 'simple': 41,\n",
       " 'cup': 25,\n",
       " 'coffee': 46,\n",
       " 'everyone': 115,\n",
       " 'freaking': 12,\n",
       " 'truly': 20,\n",
       " 'red': 80,\n",
       " 'curry': 11,\n",
       " 'leftovers': 6,\n",
       " 'one': 540,\n",
       " 'powerful': 53,\n",
       " 'forces': 33,\n",
       " 'change': 180,\n",
       " 'girl': 109,\n",
       " 'does': 107,\n",
       " 'green': 43,\n",
       " 'take': 180,\n",
       " 'game': 152,\n",
       " 'level': 35,\n",
       " 'area': 490,\n",
       " 'said': 39,\n",
       " 'sorry': 25,\n",
       " 'ryan': 84,\n",
       " 'lochte': 5,\n",
       " 'apologizes': 39,\n",
       " 'behavior': 20,\n",
       " 'rio': 8,\n",
       " 'dakota': 18,\n",
       " 'heard': 30,\n",
       " 'hours': 70,\n",
       " '4': 167,\n",
       " 'lessons': 56,\n",
       " 'prison': 61,\n",
       " 'taught': 18,\n",
       " 'me': 132,\n",
       " 'about': 1056,\n",
       " 'power': 99,\n",
       " 'control': 91,\n",
       " 'sick': 49,\n",
       " 'line': 100,\n",
       " 'body': 113,\n",
       " 'americas': 62,\n",
       " 'first': 435,\n",
       " 'really': 231,\n",
       " 'vicious': 5,\n",
       " 'process': 25,\n",
       " 'contains': 11,\n",
       " 'solutions': 9,\n",
       " 'time': 433,\n",
       " 'congressman': 62,\n",
       " 'thinks': 97,\n",
       " 'can': 445,\n",
       " 'fix': 27,\n",
       " 'economy': 41,\n",
       " 'drinking': 27,\n",
       " 'beer': 36,\n",
       " 'breast': 18,\n",
       " 'implants': 7,\n",
       " 'found': 106,\n",
       " 'cause': 25,\n",
       " 'problems': 42,\n",
       " 'mice': 5,\n",
       " 'orders': 34,\n",
       " 'strikes': 22,\n",
       " 'chemical': 15,\n",
       " 'attack': 115,\n",
       " 'ceos': 11,\n",
       " 'funeral': 21,\n",
       " 'dream': 70,\n",
       " 'round': 14,\n",
       " 'it': 791,\n",
       " '90': 26,\n",
       " 'teachers': 39,\n",
       " 'pledge': 15,\n",
       " 'culture': 42,\n",
       " 'abuse': 44,\n",
       " 'nypd': 13,\n",
       " 'weighs': 11,\n",
       " 'allowing': 13,\n",
       " 'following': 57,\n",
       " 'eric': 37,\n",
       " 'garner': 7,\n",
       " 'bono': 5,\n",
       " 'foundation': 22,\n",
       " 'prevents': 5,\n",
       " 'risk': 49,\n",
       " 'youths': 5,\n",
       " 'trees': 8,\n",
       " 'pope': 109,\n",
       " 'francis': 65,\n",
       " 'reminds': 37,\n",
       " 'caring': 9,\n",
       " 'everyones': 7,\n",
       " 'responsibility': 12,\n",
       " '6': 155,\n",
       " 'old': 387,\n",
       " 'when': 296,\n",
       " 'told': 61,\n",
       " 'kitten': 7,\n",
       " 'boyfriend': 36,\n",
       " 'thought': 73,\n",
       " 'hed': 15,\n",
       " 'check': 46,\n",
       " 'throw': 26,\n",
       " 'day': 465,\n",
       " 'off': 383,\n",
       " 'doctors': 38,\n",
       " 'restore': 12,\n",
       " 'ken': 13,\n",
       " 'burns': 16,\n",
       " 'full': 93,\n",
       " 'color': 40,\n",
       " 'vision': 24,\n",
       " 'removing': 7,\n",
       " 'tumor': 6,\n",
       " 'visual': 6,\n",
       " '12': 91,\n",
       " 'slave': 6,\n",
       " 'tie': 9,\n",
       " '2014': 46,\n",
       " 'awards': 50,\n",
       " 'coming': 101,\n",
       " '150': 11,\n",
       " 'lgbtq': 41,\n",
       " 'people': 377,\n",
       " 'last': 227,\n",
       " 'oliver': 13,\n",
       " 'lays': 10,\n",
       " 'disturbing': 13,\n",
       " 'ways': 126,\n",
       " 'which': 44,\n",
       " 'america': 175,\n",
       " 'house': 352,\n",
       " 'gop': 249,\n",
       " 'crackdown': 12,\n",
       " 'continues': 29,\n",
       " 'sure': 84,\n",
       " 'skills': 23,\n",
       " 'please': 28,\n",
       " 'my': 321,\n",
       " 'grandson': 9,\n",
       " 'says': 453,\n",
       " 'queen': 40,\n",
       " 'elizabeth': 39,\n",
       " 'before': 272,\n",
       " 'meghan': 13,\n",
       " 'markle': 8,\n",
       " 'visiting': 25,\n",
       " 'hometown': 14,\n",
       " 'find': 105,\n",
       " 'childhood': 17,\n",
       " 'still': 332,\n",
       " 'there': 122,\n",
       " 'look': 168,\n",
       " 'attacked': 6,\n",
       " 'bug': 5,\n",
       " 'clinton': 309,\n",
       " 'becomes': 32,\n",
       " 'president': 220,\n",
       " '18': 41,\n",
       " 'feet': 22,\n",
       " 'pole': 6,\n",
       " 'hunter': 7,\n",
       " 's': 467,\n",
       " 'thompson': 10,\n",
       " 'shoots': 19,\n",
       " 'mouth': 31,\n",
       " 'grand': 16,\n",
       " 'climate': 130,\n",
       " 'conspiracy': 18,\n",
       " 'theory': 31,\n",
       " 'barbra': 5,\n",
       " 'streisand': 5,\n",
       " '5': 301,\n",
       " 'questions': 66,\n",
       " 'wish': 34,\n",
       " 'younger': 19,\n",
       " 'asking': 42,\n",
       " 'shooter': 21,\n",
       " 'loose': 13,\n",
       " 'gunman': 17,\n",
       " 'everything': 80,\n",
       " 'flood': 9,\n",
       " 'demi': 6,\n",
       " 'lovato': 6,\n",
       " 'drops': 24,\n",
       " 'emotional': 41,\n",
       " 'music': 82,\n",
       " 'miley': 13,\n",
       " 'cyrus': 15,\n",
       " 'hemsworth': 8,\n",
       " 'notices': 5,\n",
       " 'jealous': 7,\n",
       " 'gps': 22,\n",
       " 'clearly': 43,\n",
       " 'parks': 13,\n",
       " 'comes': 75,\n",
       " 'as': 775,\n",
       " 'queer': 55,\n",
       " 'italy': 14,\n",
       " '2015': 61,\n",
       " 'cindy': 5,\n",
       " 'daughter': 70,\n",
       " 'lands': 15,\n",
       " 'major': 44,\n",
       " 'fashion': 52,\n",
       " 'l': 19,\n",
       " 'grants': 8,\n",
       " 'james': 98,\n",
       " 'corden': 20,\n",
       " 'david': 56,\n",
       " 'outfits': 8,\n",
       " 'cafÃ©': 9,\n",
       " 'adds': 48,\n",
       " 'heartbreaking': 16,\n",
       " 'little': 183,\n",
       " 'lunch': 31,\n",
       " 'menu': 12,\n",
       " 'mcdonalds': 21,\n",
       " '100': 61,\n",
       " 'percent': 69,\n",
       " 'kris': 10,\n",
       " 'jenner': 40,\n",
       " 'turned': 21,\n",
       " 'drunken': 8,\n",
       " 'valentines': 29,\n",
       " 'karaoke': 8,\n",
       " 'whale': 12,\n",
       " 'regrets': 14,\n",
       " '000': 189,\n",
       " 'plastic': 22,\n",
       " 'chips': 7,\n",
       " 'fell': 8,\n",
       " 'container': 8,\n",
       " 'ship': 15,\n",
       " 'texas': 76,\n",
       " 'ebola': 41,\n",
       " 'patient': 17,\n",
       " 'fighting': 49,\n",
       " 'too': 198,\n",
       " 'late': 59,\n",
       " 'perfect': 83,\n",
       " 'actor': 39,\n",
       " 'role': 44,\n",
       " 'head': 81,\n",
       " 'room': 88,\n",
       " 'producers': 13,\n",
       " 'were': 118,\n",
       " 'giving': 63,\n",
       " 'hope': 57,\n",
       " 'robert': 41,\n",
       " 'de': 23,\n",
       " 'turn': 52,\n",
       " '58': 7,\n",
       " 'movie': 112,\n",
       " 'pruitt': 16,\n",
       " 'sort': 24,\n",
       " 'answers': 13,\n",
       " 'whether': 45,\n",
       " 'believes': 18,\n",
       " 'accused': 55,\n",
       " 'father': 80,\n",
       " 'son': 111,\n",
       " 'thieves': 5,\n",
       " 'cops': 57,\n",
       " 'slow': 18,\n",
       " 'youll': 33,\n",
       " 'fine': 23,\n",
       " 'drunk': 55,\n",
       " 'driver': 37,\n",
       " 'away': 119,\n",
       " 'stolen': 16,\n",
       " 'police': 234,\n",
       " 'car': 101,\n",
       " 'fuel': 9,\n",
       " '7': 159,\n",
       " 'super': 86,\n",
       " 'pac': 15,\n",
       " 'democratic': 61,\n",
       " 'primary': 36,\n",
       " 'air': 72,\n",
       " 'force': 47,\n",
       " 'no': 415,\n",
       " 'longer': 47,\n",
       " 'require': 8,\n",
       " 'so': 245,\n",
       " 'god': 127,\n",
       " 'paul': 118,\n",
       " 'dies': 77,\n",
       " 'consuming': 7,\n",
       " 'eggs': 13,\n",
       " 'worried': 50,\n",
       " 'dealer': 6,\n",
       " 'whos': 34,\n",
       " 'picking': 5,\n",
       " 'phone': 72,\n",
       " 'dad': 145,\n",
       " 'hotel': 38,\n",
       " '10': 224,\n",
       " 'miles': 22,\n",
       " 'city': 121,\n",
       " 'youre': 78,\n",
       " 'explosion': 10,\n",
       " 'building': 45,\n",
       " 'outside': 61,\n",
       " 'paris': 47,\n",
       " 'least': 55,\n",
       " '2': 214,\n",
       " 'military': 67,\n",
       " 'finally': 131,\n",
       " 'revealed': 15,\n",
       " 'union': 57,\n",
       " 'sanders': 110,\n",
       " 'staffers': 19,\n",
       " 'members': 47,\n",
       " 'influence': 13,\n",
       " 'workers': 55,\n",
       " 'seattle': 15,\n",
       " 'sex': 156,\n",
       " 'lawsuit': 35,\n",
       " 'southern': 8,\n",
       " 'comfort': 11,\n",
       " 'comforts': 8,\n",
       " 'featured': 6,\n",
       " 'fox': 60,\n",
       " 'they': 253,\n",
       " 'edited': 6,\n",
       " 'plus': 17,\n",
       " 'size': 22,\n",
       " 'models': 11,\n",
       " 'runway': 8,\n",
       " 'baltimore': 14,\n",
       " 'inspire': 10,\n",
       " 'inner': 18,\n",
       " 'spirit': 17,\n",
       " 'jewish': 20,\n",
       " 'boy': 87,\n",
       " 'dreams': 36,\n",
       " 'being': 230,\n",
       " 'only': 236,\n",
       " 'parenting': 28,\n",
       " 'advice': 47,\n",
       " 'id': 18,\n",
       " 'dare': 6,\n",
       " 'family': 275,\n",
       " 'chooses': 8,\n",
       " 'different': 62,\n",
       " 'dog': 144,\n",
       " 'than': 323,\n",
       " 'grandfather': 10,\n",
       " 'celebrities': 25,\n",
       " 'celebrate': 42,\n",
       " 'fourth': 29,\n",
       " 'july': 14,\n",
       " 'some': 182,\n",
       " 'fun': 61,\n",
       " 'sun': 20,\n",
       " 'international': 28,\n",
       " 'womens': 73,\n",
       " 'western': 12,\n",
       " 'save': 82,\n",
       " 'michelle': 44,\n",
       " 'beauty': 45,\n",
       " 'caitlyn': 15,\n",
       " 'responds': 39,\n",
       " 'golden': 35,\n",
       " 'globes': 18,\n",
       " 'jokes': 18,\n",
       " 'joy': 22,\n",
       " 'michael': 76,\n",
       " 'flynn': 11,\n",
       " 'guilty': 31,\n",
       " 'plea': 15,\n",
       " 'pure': 12,\n",
       " 'things': 217,\n",
       " 'tells': 78,\n",
       " 'their': 272,\n",
       " 'weight': 46,\n",
       " 'loss': 38,\n",
       " 'journey': 23,\n",
       " 'fan': 66,\n",
       " 'disappointed': 26,\n",
       " 'learn': 41,\n",
       " 'ron': 12,\n",
       " 'marriage': 94,\n",
       " 'breaks': 51,\n",
       " 'differences': 8,\n",
       " 'yo': 11,\n",
       " 'helps': 28,\n",
       " 'raise': 34,\n",
       " 'california': 78,\n",
       " 'engineering': 5,\n",
       " 'warren': 27,\n",
       " 'racist': 41,\n",
       " 'bully': 11,\n",
       " 'six': 37,\n",
       " 'hurt': 27,\n",
       " 'bus': 50,\n",
       " 'crash': 49,\n",
       " 'problem': 64,\n",
       " 'heads': 26,\n",
       " 'kim': 72,\n",
       " 'gordon': 8,\n",
       " 'clarifies': 8,\n",
       " 'her': 395,\n",
       " 'comments': 36,\n",
       " 'del': 7,\n",
       " 'feminism': 13,\n",
       " 'bashar': 6,\n",
       " 'al': 59,\n",
       " 'assad': 13,\n",
       " 'tries': 33,\n",
       " 'tiny': 25,\n",
       " 'bit': 18,\n",
       " 'gas': 31,\n",
       " 'cancer': 73,\n",
       " 'im': 93,\n",
       " 'arrested': 41,\n",
       " 'charged': 27,\n",
       " 'driving': 51,\n",
       " 'need': 195,\n",
       " 'know': 224,\n",
       " 'controversy': 24,\n",
       " 'erupts': 9,\n",
       " 'uk': 15,\n",
       " 'removes': 14,\n",
       " 'gender': 30,\n",
       " 'labels': 5,\n",
       " 'clothes': 22,\n",
       " 'sixth': 15,\n",
       " 'grade': 26,\n",
       " 'class': 81,\n",
       " 'insurance': 31,\n",
       " 'salesman': 10,\n",
       " 'celebrates': 58,\n",
       " 'friends': 132,\n",
       " 'always': 78,\n",
       " 'best': 244,\n",
       " 'neil': 19,\n",
       " 'watch': 233,\n",
       " 'another': 146,\n",
       " 'sci': 8,\n",
       " 'fi': 11,\n",
       " 'friend': 107,\n",
       " 'biologists': 5,\n",
       " 'announce': 36,\n",
       " 'theyre': 54,\n",
       " 'done': 39,\n",
       " 'megyn': 8,\n",
       " 'kelly': 38,\n",
       " 'story': 123,\n",
       " 'project': 30,\n",
       " 'going': 213,\n",
       " 'viral': 30,\n",
       " 'creating': 21,\n",
       " 'where': 158,\n",
       " 'none': 7,\n",
       " 'seems': 21,\n",
       " 'exist': 12,\n",
       " 'taps': 9,\n",
       " 'love': 211,\n",
       " 'rapping': 5,\n",
       " 'newspaper': 23,\n",
       " 'worry': 15,\n",
       " 'spending': 33,\n",
       " 'much': 137,\n",
       " 'healthy': 38,\n",
       " 'against': 198,\n",
       " 'lives': 74,\n",
       " 'matter': 39,\n",
       " 'central': 15,\n",
       " 'meaning': 15,\n",
       " 'amendment': 18,\n",
       " 'vatican': 28,\n",
       " 'putting': 35,\n",
       " 'public': 99,\n",
       " 'react': 9,\n",
       " 'wire': 10,\n",
       " 'cast': 31,\n",
       " 'reunited': 8,\n",
       " 'community': 63,\n",
       " 'obamas': 68,\n",
       " 'supreme': 116,\n",
       " 'court': 179,\n",
       " 'nominee': 27,\n",
       " 'sending': 16,\n",
       " 'free': 118,\n",
       " 'd': 53,\n",
       " 'c': 53,\n",
       " 'security': 92,\n",
       " 'guard': 25,\n",
       " 'custody': 12,\n",
       " 'ruled': 6,\n",
       " 'homicide': 7,\n",
       " 'leave': 62,\n",
       " 'clean': 37,\n",
       " 'makes': 175,\n",
       " 'minute': 58,\n",
       " 'push': 43,\n",
       " 'appeal': 12,\n",
       " 'whites': 10,\n",
       " 'facing': 12,\n",
       " 'rising': 21,\n",
       " 'remote': 13,\n",
       " 'village': 11,\n",
       " 'votes': 32,\n",
       " 'move': 64,\n",
       " 'childs': 29,\n",
       " 'memories': 11,\n",
       " 'trying': 118,\n",
       " 'worst': 50,\n",
       " 'place': 69,\n",
       " 'child': 168,\n",
       " 'sentenced': 19,\n",
       " 'encouraging': 10,\n",
       " 'boyfriends': 10,\n",
       " 'assault': 63,\n",
       " 'baby': 146,\n",
       " 'values': 11,\n",
       " 'march': 67,\n",
       " 'inspired': 33,\n",
       " 'them': 141,\n",
       " 'run': 102,\n",
       " 'men': 124,\n",
       " 'worth': 47,\n",
       " 'senators': 38,\n",
       " 'arent': 37,\n",
       " 'accept': 15,\n",
       " 'champion': 10,\n",
       " 'price': 33,\n",
       " 'takes': 162,\n",
       " 'failing': 23,\n",
       " 'harsh': 6,\n",
       " 'past': 74,\n",
       " 'bacon': 14,\n",
       " 'toaster': 5,\n",
       " 'completes': 14,\n",
       " 'documentary': 39,\n",
       " 'fucking': 68,\n",
       " 'liars': 7,\n",
       " 'watched': 14,\n",
       " 'jazz': 11,\n",
       " 'magic': 22,\n",
       " 'fail': 25,\n",
       " 'rice': 14,\n",
       " 'cake': 24,\n",
       " 'stunning': 34,\n",
       " 'surprise': 48,\n",
       " 'carly': 13,\n",
       " 'rae': 8,\n",
       " 'released': 31,\n",
       " 'answer': 23,\n",
       " 'jeopardy': 7,\n",
       " 'clue': 8,\n",
       " 'female': 84,\n",
       " 'getting': 185,\n",
       " 'lot': 63,\n",
       " 'avoiding': 5,\n",
       " 'touching': 19,\n",
       " 'male': 43,\n",
       " 'patrons': 13,\n",
       " 'hands': 50,\n",
       " 'kills': 50,\n",
       " 'four': 39,\n",
       " 'german': 17,\n",
       " 'nightclub': 8,\n",
       " 'couple': 127,\n",
       " 'point': 53,\n",
       " 'comfortable': 15,\n",
       " 'using': 57,\n",
       " 'toilet': 16,\n",
       " 'same': 102,\n",
       " 'mountain': 12,\n",
       " 'lion': 10,\n",
       " 'scientists': 79,\n",
       " 'near': 48,\n",
       " 'road': 48,\n",
       " 'noah': 29,\n",
       " 'debut': 11,\n",
       " 'cry': 13,\n",
       " 'scalia': 16,\n",
       " 'goes': 90,\n",
       " 'abortion': 63,\n",
       " 'passed': 12,\n",
       " 'justice': 78,\n",
       " 'big': 205,\n",
       " 'sized': 6,\n",
       " 'finish': 21,\n",
       " 'those': 61,\n",
       " 'fries': 7,\n",
       " 'taylor': 55,\n",
       " 'swift': 38,\n",
       " 'voice': 48,\n",
       " 'writing': 31,\n",
       " '1989': 6,\n",
       " ...}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING PHASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Probabilities(data):\n",
    "    probabities = {}\n",
    "    \n",
    "    probabities[1] = data['is_sarcastic'].value_counts()[1]\n",
    "    probabities[0] = data['is_sarcastic'].value_counts()[0]\n",
    "    \n",
    "    return probabities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniqueWords(train, vocab):\n",
    "    words = {}\n",
    "    \n",
    "    for line in train['headline']:\n",
    "        line = line.split()\n",
    "        \n",
    "        for word in line:\n",
    "            if word in vocab:\n",
    "                words[word] = 1\n",
    "            \n",
    "    return list(words.keys())            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countBoth(X, words):                           #Method to count the total sarcastic sentence and non-sarcastic sentences\n",
    "                                               #X is the trained dataset\n",
    "    sarcastic_sent = 0\n",
    "    non_sarc_sent = 0\n",
    "    \n",
    "    for i in range(0, len(X)):\n",
    "        line = X.iloc[i]\n",
    "        tag = line[1]\n",
    "        line = line[0].split()\n",
    "        \n",
    "        for word in line:\n",
    "            if tag == 1:\n",
    "                sarcastic_sent += 1\n",
    "            else:\n",
    "                non_sarc_sent += 1\n",
    "    \n",
    "    return sarcastic_sent, non_sarc_sent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordsOccurences(train, words):                     #Method would calculate the number of times a word appeared in sarcastic as well as non-sarcastic sentence\n",
    "\n",
    "    count_words = {}\n",
    "    \n",
    "    for i in range(0, len(train)):\n",
    "        line = train.iloc[i]\n",
    "        tag = line[1]\n",
    "        line = line[0].split()\n",
    "        \n",
    "        for word in line:\n",
    "            if word in words and word not in count_words:\n",
    "                count_words[word] = {}\n",
    "                count_words[word][0] = 1\n",
    "                count_words[word][1] = 1\n",
    " \n",
    "            elif tag == 1 and word in count_words:\n",
    "                count_words[word][1] += 1\n",
    "            \n",
    "            elif tag == 0 and word in count_words:\n",
    "                count_words[word][0] += 1\n",
    "                \n",
    "    \n",
    "    return count_words     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_probs(train, sarc, non_sarc, word_count, total_unique):\n",
    "    prob = {}\n",
    "    p = Probabilities(train)\n",
    "    prob['pr_0'] = p[0]\n",
    "    prob['pr_1'] = p[1]\n",
    "\n",
    "    prob['total_w'] = total_unique\n",
    "    prob['tag0'] = non_sarc\n",
    "    prob['tag1'] = sarc\n",
    "    \n",
    "    for word in word_count:\n",
    "        prob[word] = {}\n",
    "        prob[word][0] = ((word_count[word][0] + 1) / (total_unique + non_sarc))\n",
    "        prob[word][1] = ((word_count[word][1] + 1) / (total_unique + sarc))\n",
    "    \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NaiveBayes(train, vocab):\n",
    "    words = uniqueWords(train, vocab)\n",
    "    \n",
    "    total_words = len(words)\n",
    "    \n",
    "    sarc, non_sarc = countBoth(train, words)\n",
    "    \n",
    "    word_count = wordsOccurences(train, words)\n",
    "    \n",
    "    prob = find_probs(train, sarc, non_sarc, word_count, total_words)\n",
    "    \n",
    "    return prob\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = train_test_split(data, test_size = 0.3,shuffle=True)      #70% training and 30% testing\n",
    "\n",
    "X.reset_index(inplace=True, drop=True)\n",
    "\n",
    "Y.reset_index(inplace=True, drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>once homeless student who worked 4 jobs to sup...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>james comeys book pre sold almost 200 000 copi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>syrian archbishop on christians threatened by ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>whoops  franklin grahams new bank is lgbt frie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chris christie neglects new jersey woes while ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18691</th>\n",
       "      <td>grieving couple finds different ways to use st...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18692</th>\n",
       "      <td>kim kardashian celebrates 42 million insta fol...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18693</th>\n",
       "      <td>someone filming b roll at pike place market ri...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18694</th>\n",
       "      <td>local man knows he moved to minneapolis for so...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18695</th>\n",
       "      <td>bidens buffalo wing challenge dinner not sitti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18696 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                headline  is_sarcastic\n",
       "0      once homeless student who worked 4 jobs to sup...             0\n",
       "1      james comeys book pre sold almost 200 000 copi...             0\n",
       "2      syrian archbishop on christians threatened by ...             0\n",
       "3      whoops  franklin grahams new bank is lgbt frie...             0\n",
       "4      chris christie neglects new jersey woes while ...             0\n",
       "...                                                  ...           ...\n",
       "18691  grieving couple finds different ways to use st...             1\n",
       "18692  kim kardashian celebrates 42 million insta fol...             0\n",
       "18693  someone filming b roll at pike place market ri...             1\n",
       "18694  local man knows he moved to minneapolis for so...             1\n",
       "18695  bidens buffalo wing challenge dinner not sitti...             1\n",
       "\n",
       "[18696 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training Dataset')\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataSet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>watch this cat lose its mind after faced with ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>president trumps war on children</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the case for holistic education in the wake of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>khloe kardashian thanks fans for their patienc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>new therapist obsessed with old therapist</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8008</th>\n",
       "      <td>new report finds americans most interested in ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8009</th>\n",
       "      <td>a sadder pride because of washington inaction</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8010</th>\n",
       "      <td>rob kardashian apparently tweeted kylie jenner...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8011</th>\n",
       "      <td>man humiliated by wi fis poor behavior in fron...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8012</th>\n",
       "      <td>fiscal challenges for nycs health and hospital...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8013 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               headline  is_sarcastic\n",
       "0     watch this cat lose its mind after faced with ...             0\n",
       "1                      president trumps war on children             0\n",
       "2     the case for holistic education in the wake of...             0\n",
       "3     khloe kardashian thanks fans for their patienc...             0\n",
       "4             new therapist obsessed with old therapist             1\n",
       "...                                                 ...           ...\n",
       "8008  new report finds americans most interested in ...             1\n",
       "8009      a sadder pride because of washington inaction             0\n",
       "8010  rob kardashian apparently tweeted kylie jenner...             0\n",
       "8011  man humiliated by wi fis poor behavior in fron...             1\n",
       "8012  fiscal challenges for nycs health and hospital...             0\n",
       "\n",
       "[8013 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Testing DataSet')\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Training Dataset:  18696\n",
      "Length of Testing dataset 8013\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of Training Dataset: \", len(X))\n",
    "print(\"Length of Testing dataset\", len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NaiveBayes(X, words_tf.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VSM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_idf(document_frequency):\n",
    "    idf = {}\n",
    "    for i in document_frequency:\n",
    "        \n",
    "        for j in document_frequency[i]:\n",
    "            if i not in idf:\n",
    "                idf[i] = 0\n",
    "            idf[i] += 1 \n",
    "        \n",
    "        if(idf[i] > 0):\n",
    "            idf[i] = 1/idf[i]\n",
    "    return idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess():\n",
    "    doc_vector = [dict() for x in range(51)]\n",
    "    document_frequency = {}\n",
    "\n",
    "    for i in range(1,51):\n",
    "        for word in X.iloc[i,0].split():       \n",
    "            #for word in line.split():\n",
    "                if(True):\n",
    "                    #word = clean(word)\n",
    "                    #word = lemmatizer.lemmatize(word, pos =  \"v\")\n",
    "                    if word not in document_frequency:\n",
    "                        document_frequency[word] = []\n",
    "                   \n",
    "                    if word in document_frequency:\n",
    "                        if i not in document_frequency[word]:\n",
    "                            document_frequency[word].append(i)\n",
    "                    \n",
    "                    if word not in doc_vector[i]:\n",
    "                        doc_vector[i][word] = 0\n",
    "                    \n",
    "                    if word in doc_vector[i]:\n",
    "                        doc_vector[i][word] += 1\n",
    "    \n",
    "\n",
    "    idf = calc_idf(document_frequency)\n",
    "    \n",
    "    for i in range(1,51):\n",
    "        for x in doc_vector[i]:\n",
    "            doc_vector[i][x] = doc_vector[i][x] * idf[x]\n",
    "        \n",
    "    \n",
    "    return idf, doc_vector, document_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "idf, doc_vector, document_frequency = preProcess()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mod(vector):\n",
    "    sum = 0\n",
    "    for i in vector:\n",
    "        sum += (vector[i] * vector[i])\n",
    "    return math.sqrt(sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dot_prod(v1, v2):\n",
    "    cp = 0\n",
    "    for i in v1:\n",
    "        if i in v2:\n",
    "            cp += v1[i] * v2[i]\n",
    "    return cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_sarcastic_vsm(sentence):\n",
    "    score = -1\n",
    "    index =0 \n",
    "    count = 0\n",
    "    \n",
    "    for i in doc_vector:\n",
    "        s1 = dot_prod(sentence, i)\n",
    "        #s = score/(get_mod(sentence) * get_mod(i))\n",
    "        if s1 >= score:\n",
    "            score = s1\n",
    "            index = count\n",
    "        count += 1\n",
    "    \n",
    "    return X.iloc[index, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def n_gram_apply():\n",
    "    ss= len(X)\n",
    "    check = False\n",
    "    for i in range(0,ss, 10):\n",
    "        if(X.iloc[i,1] == 1 or check == True):\n",
    "            if(X.iloc[i,1] == 1):\n",
    "                check = True\n",
    "            else:\n",
    "                check = False\n",
    "            #comment=eliminate_punct(data.iloc[i,0])\n",
    "            ngram_vocab = ngrams(X.iloc[i,0].split(), 4)\n",
    "            #print(data.iloc[i,0])\n",
    "            new_df = {}\n",
    "            for j in ngram_vocab:\n",
    "                sentence = \"\"\n",
    "                for k in j:\n",
    "                    sentence = sentence + \" \" + str(k.lower())\n",
    "                sentence = eliminate_punct(sentence)\n",
    "                new_df['headline'] = sentence\n",
    "                new_df['is_sarcastic'] = X.iloc[i,1]\n",
    "                X=X.append(new_df, ignore_index = True)\n",
    "                new_df.clear()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING PHASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_NaiveBayes(trained,test):\n",
    "    result_list = []\n",
    "    tf = 1\n",
    "\n",
    "    for i in range(0, len(test)):\n",
    "        predicted = \"\"\n",
    "        true_prob = 1\n",
    "        false_prob = 1\n",
    "        line = test.iloc[i]\n",
    "        line = line[0].split()\n",
    "        \n",
    "        for w in line:\n",
    "            if w in trained:\n",
    "                true_prob *= trained[w][1]\n",
    "                false_prob *= trained[w][0]\n",
    "            else:\n",
    "                true_prob *= (1 + tf) / ((trained['total_w'] + trained['tag1']))\n",
    "                false_prob *= (1 + tf) / ((trained['total_w'] + trained['tag0']))\n",
    "        \n",
    "        true_prob = (true_prob * trained['pr_1'])\n",
    "        false_prob = (false_prob * trained['pr_0'])\n",
    "        \n",
    "        if true_prob > false_prob:\n",
    "            predicted = 1\n",
    "        else:\n",
    "            predicted = 0\n",
    "        \n",
    "        result_list.append(predicted)\n",
    "    \n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestVSM():\n",
    "    sentence = {}                                  #Testing the VSM with the sentence \"report\"\n",
    "    for i in sent.split():\n",
    "        sentence[i] = 1\n",
    "    #print(cross_prod(sentence, X.iloc[0,0]))\n",
    "    print(\"Ans: \")\n",
    "    print(is_sarcastic_vsm(sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = test_NaiveBayes(train_dataset,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TestVSM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the Accuracy of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Accuracy(actual, predicted):\n",
    "    true_predicted = 0\n",
    "    \n",
    "    for i in range(0, len(predicted)):\n",
    "        if(i < len(actual) and i < len(predicted) and actual[i] == predicted[i]):\n",
    "            true_predicted += 1\n",
    "    \n",
    "    accuracy = (true_predicted / len(Y)) * 100\n",
    "    \n",
    "    return accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsm_predicted = []\n",
    "\n",
    "def calc_vsm_accuracy():\n",
    "    for i in range(0, 8013):\n",
    "        sent = {}\n",
    "        for w in Y.iloc[i,0].split():\n",
    "            if w not in sent:\n",
    "                sent[w] = 0\n",
    "            sent[w] +=1\n",
    "        vsm_predicted.append(is_sarcastic_vsm(sent))\n",
    "        sent.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = list(Y['is_sarcastic'])\n",
    "naiveBayes_acc = Accuracy(actual, predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VSM Accuracy is :  53.36328466242356\n",
      "VSM Rounded-Accurcy 54\n"
     ]
    }
   ],
   "source": [
    "\n",
    "calc_vsm_accuracy()\n",
    "vsm_acc = Accuracy(actual, vsm_predicted)\n",
    "print(\"VSM Accuracy is : \", vsm_acc)\n",
    "print(\"VSM Rounded-Accurcy\",math.ceil(vsm_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy is :  84.89953825034318\n",
      "Naive Bayes Rounded-Accurcy 85\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes Accuracy is : \", naiveBayes_acc)\n",
    "print(\"Naive Bayes Rounded-Accurcy\", math.ceil(naiveBayes_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_output = Y.copy()\n",
    "predict_output = predict_output.drop(['is_sarcastic'],axis=1)\n",
    "predict_output['is_sarcastic'] = predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Predicted Output on test data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>watch this cat lose its mind after faced with ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>president trumps war on children</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the case for holistic education in the wake of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>khloe kardashian thanks fans for their patienc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>new therapist obsessed with old therapist</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8008</th>\n",
       "      <td>new report finds americans most interested in ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8009</th>\n",
       "      <td>a sadder pride because of washington inaction</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8010</th>\n",
       "      <td>rob kardashian apparently tweeted kylie jenner...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8011</th>\n",
       "      <td>man humiliated by wi fis poor behavior in fron...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8012</th>\n",
       "      <td>fiscal challenges for nycs health and hospital...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8013 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               headline  is_sarcastic\n",
       "0     watch this cat lose its mind after faced with ...             0\n",
       "1                      president trumps war on children             0\n",
       "2     the case for holistic education in the wake of...             0\n",
       "3     khloe kardashian thanks fans for their patienc...             0\n",
       "4             new therapist obsessed with old therapist             1\n",
       "...                                                 ...           ...\n",
       "8008  new report finds americans most interested in ...             1\n",
       "8009      a sadder pride because of washington inaction             0\n",
       "8010  rob kardashian apparently tweeted kylie jenner...             0\n",
       "8011  man humiliated by wi fis poor behavior in fron...             1\n",
       "8012  fiscal challenges for nycs health and hospital...             0\n",
       "\n",
       "[8013 rows x 2 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('The Predicted Output on test data')\n",
    "predict_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison Of Accuracies of both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 84.89953825034318\n",
      "VSM Accuracy: 53.36328466242356\n"
     ]
    }
   ],
   "source": [
    "print('Naive Bayes Accuracy:', naiveBayes_acc)\n",
    "print('VSM Accuracy:', vsm_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The Naive Bayes Gained the accuracy of 85%, whereas The VSM gained the accuracy of 58%. The Naive Bayes model worked very well on the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify Based on user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isSarcastic_NaiveBayes(trained, sent):\n",
    "    line=sent.split()\n",
    "    true_prob=1\n",
    "    false_prob=1\n",
    "    tf=1\n",
    "    \n",
    "    for w in line:\n",
    "        if w in trained:\n",
    "            true_prob *= trained[w][1]\n",
    "            false_prob *= trained[w][0]\n",
    "        else:\n",
    "            true_prob *= (1+tf) / ((trained['total_w'] + trained['tag1']))\n",
    "            false_prob *= (1+tf) / ((trained['total_w'] + trained['tag0']))\n",
    "    \n",
    "    \n",
    "    true_prob = (true_prob * trained['pr_1'])\n",
    "    false_prob = (false_prob * trained['pr_0'])\n",
    "    \n",
    "    if(true_prob > false_prob):\n",
    "        return \"Sarcastic\"\n",
    "    else:\n",
    "        return \"Not Sarcastic\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_input=input(\"Enter The Sentence : \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_input = eliminate_punct(sent_input)\n",
    "sent_input = remove_stop_words(sent_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = isSarcastic_NaiveBayes(train_dataset,sent_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
